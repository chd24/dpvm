/* neuron class */
char methodVersion[] = "genNetTraining() internal method, T19.498-T19.744"; /* $DVS:time$ */

#include "../../../stdlib/stdlib.dpvmake"
#include "../../../utils/utils.dpvmake"
#include "neuron.dpvmake"

(volatile any obj) -> (volatile any subobj) popObject = {
	any subobj = obj.l[obj.lsize - 1];
	obj.lpop(1);
	return subobj;
};

(const any obj) -> (volatile any obj) unfixObject = {
	asm {
		unfix
		ret
	};
};

(volatile stateful neuronData data) -> (const char error[]) genNetTraining = {
	neuronNet net = data.cdata.net;
	neuronNetRuntime netR = data.net;
	int flags = data.cdata.flags;


	/* generate structure netData holding all float arrays */

	char t[];
	printf(t, "type netData = {\r\n", {});
	int i, j, k, last = net.layers.lsize - 1, coeff = 1;
	if (flags & NEURON_METHOD_RMSP)
		coeff = 3;
	else if (flags & NEURON_METHOD_MOMENTUM)
		coeff = 2;
	for (i = 0; i <= last; i += 1) {
		neuronLayer layer = net.layers[i];
		for (j = 0; j < NEURON_PARALLEL; j += 1) {
			char c = 'a' + j, C = 'A' + j, d = 'q' + j;
			printf(t, "\tvolatile float %c%d[%d][0..1];\r\n", {i, layer.groups[j].nOutputs, c});
			printf(t, "\tvolatile float %c%d[%d][0..1];\r\n", {i, layer.groups[j].nOutputs, C});
			printf(t, "\tvolatile float %c%d[%d][0..1];\r\n", {i, layer.groups[j].nWeights * coeff, d});
		}
	}
	printf(t, "};\r\n", {});


	/* generate source code for group compute function, part 1 */

	char s[];
	printf(s, "/* Source of netTraining, generated by %s */
%s
(volatile stateful any data, volatile stateful float outputs[], const float params[%d]) -> (float res) training = {
	netData r[]; %p(data,r); netData t = r[0];
", {methodVersion, t, pushObject, trainingParams.i[10]});

	any links[], funcs[NEURON_PARALLEL..];
	links.lpush(pushObject);

	int mode;
	for (mode = 0; mode < 2; mode += 1) {
	    int begin, end, step;
	    if (mode) begin = last - 1, end = 0, step = -1;
	    else begin = 1, end = last + 1, step = 1;

	    for (i = begin; i != end; i += step) {
		neuronLayer layer = net.layers[i];
		int nSubroutines, nn[NEURON_PARALLEL], n;

		if (flags & NEURON_NO_PARALLEL) {
			for (j = 0; j < NEURON_PARALLEL; j += 1)
				(funcs[j], nn[j]) = genGroupCompute(data.cdata, i, j, mode);
		} else [
			(funcs[0], nn[0]) = genGroupCompute(data.cdata, i, 0, mode);
			(funcs[1], nn[1]) = genGroupCompute(data.cdata, i, 1, mode);
			(funcs[2], nn[2]) = genGroupCompute(data.cdata, i, 2, mode);
			(funcs[3], nn[3]) = genGroupCompute(data.cdata, i, 3, mode);
			(funcs[4], nn[4]) = genGroupCompute(data.cdata, i, 4, mode);
			(funcs[5], nn[5]) = genGroupCompute(data.cdata, i, 5, mode);
/*			(funcs[6], nn[6]) = genGroupCompute(data.cdata, i, 6, mode);
			(funcs[7], nn[7]) = genGroupCompute(data.cdata, i, 7, mode);
*/		]

		for (j = 0; j < NEURON_PARALLEL; j += 1) {
			if (funcs[j].type == "".type) {
				char res[][];
				pushObject(funcs[j], res);
				return res[0];
			}
			links.lpush(funcs[j]);
			nSubroutines += nn[j];
		}

		int parallel = !(flags & NEURON_NO_PARALLEL) && nSubroutines >= SUBROUTINES_MAX;

		if (parallel) {
			for (j = 0; j < NEURON_PARALLEL; j += 1) {
				n = layer.groups[j].nOutputs;
				printf(s, "\tconst (volatile float obj[%d][0..1]) -> (volatile float subobj[%d]) popObject%d_%d_%d = {
		float subobj[%d] = obj.l[0];
		obj.lpop(1);
		return subobj;
	};
", {n, n, mode, i, j, n});
			}
			printf(s, "\t[\r\n", {});
			links.lpush(pushObject);
			links.lpush(popObject);
			links.lpush(unfixObject);
		}

		for (j = 0; j < NEURON_PARALLEL; j += 1) {
			char c0 = 'a' + j, c1 = 'A' + j, c[] = {c0, c1};

			if (parallel)
				printf(s, "\tt.%c%d.lpush(%p(popObject%d_%d_%d(t.%c%d)",
					{funcs[j], i, mode, i, j, i, c[mode], c[mode]});
			else
				printf(s, "\t%p(t.%c%d[0]", {funcs[j], i, c[mode]});

			if (mode)
				printf(s, ",t.%c%d[0]", {i, c[0]});

			for (k = 0; k < NEURON_PARALLEL; k += 1) {
				char C = 'a' + mode * ('A' - 'a') + k;
				printf(s, ",t.%c%d[0]", {i - 1 + mode * 2, C});
			}

			for (k = 0; k < NEURON_PARALLEL; k += 1) {
				char d = 'q' + k;
				printf(s, ",t.%c%d[0]", {i + mode, d});
			}

			if (parallel)
				printf(s, "));\r\n", {});
			else
				printf(s, ");\r\n", {});
		}

		if (parallel) {
			printf(s, "\t]\r\n", {});
			for (k = 0; k < NEURON_PARALLEL; k += 1) {
				char c = 'a' + k, d = 'q' + k;
				printf(s, "\t%p(%p(%p(t.%c%d)), t.%c%d);\r\n",
					{pushObject, unfixObject, popObject, i - 1 + mode, i - 1 + mode, c, c});
				if (mode) {
					char C = 'A' + k;
					printf(s, "\t%p(%p(%p(t.%c%d)), t.%c%d);\r\n",
						{pushObject, unfixObject, popObject, i + 1, i + 1, C, C});
				}
				printf(s, "\t%p(%p(%p(t.%c%d)), t.%c%d);\r\n",
					{pushObject, unfixObject, popObject, i + mode, i + mode, d, d});
			}
		}
	    }

	    if (!mode) {
		strcat(s, "\tfloat diff, res;\r\n");
		neuronLayer layer = net.layers[last];
		for (j = 0; j < layer.nNeurons; j += 1) {
			neuron n = net.neurons[layer.neuronStartId + j];
			int groupId = n.groupId, outputId = n.outputStartId;
			char c = 'a' + groupId, C = 'A' + groupId;
			printf(s, "\tdiff = t.%c%d[0][%d] - outputs[%d];\r\n\tt.%c%d[0][%d] = diff * ",
					{last, outputId, j, last, outputId, c, C});
			char argument[];
			printf(argument, "t.%c%d[0][%d]", {last, outputId, c});
			printInlineFunction(s, n.activation.derivationFromFunction, argument);
			printf(s, ";\r\n\toutputs[%d] = t.%c%d[0][%d];\r\n\tres += diff * diff;\r\n",
					{j, last, outputId, c});
		}
	    }
	}

	for (i = last; i > 0; i -= 1) {
		neuronLayer layer = net.layers[i];
		int nSubroutines, nn[NEURON_PARALLEL], n;

		if (flags & NEURON_NO_PARALLEL) {
			for (j = 0; j < NEURON_PARALLEL; j += 1)
				(funcs[j], nn[j]) = genGroupGradient(data.cdata, i, j);
		} else [
			(funcs[0], nn[0]) = genGroupGradient(data.cdata, i, 0);
			(funcs[1], nn[1]) = genGroupGradient(data.cdata, i, 1);
			(funcs[2], nn[2]) = genGroupGradient(data.cdata, i, 2);
			(funcs[3], nn[3]) = genGroupGradient(data.cdata, i, 3);
			(funcs[4], nn[4]) = genGroupGradient(data.cdata, i, 4);
			(funcs[5], nn[5]) = genGroupGradient(data.cdata, i, 5);
/*			(funcs[6], nn[6]) = genGroupGradient(data.cdata, i, 6);
			(funcs[7], nn[7]) = genGroupGradient(data.cdata, i, 7);
*/		]

		for (j = 0; j < NEURON_PARALLEL; j += 1) {
			if (funcs[j].type == "".type) {
				char res[][];
				pushObject(funcs[j], res);
				return res[0];
			}
			links.lpush(funcs[j]);
			nSubroutines += nn[j];
		}

		int parallel = !(flags & NEURON_NO_PARALLEL) && nSubroutines >= SUBROUTINES_MAX;

		if (parallel) {
			for (j = 0; j < NEURON_PARALLEL; j += 1) {
				n = layer.groups[j].nWeights * coeff;
				printf(s, "\tconst (volatile float obj[%d][0..1]) -> (volatile float subobj[%d]) popObject2_%d_%d = {
		float subobj[%d] = obj.l[0];
		obj.lpop(1);
		return subobj;
	};
", {n, n, i, j, n});
			}
			printf(s, "\t[\r\n", {});
			links.lpush(pushObject);
			links.lpush(popObject);
			links.lpush(unfixObject);
		}

		for (j = 0; j < NEURON_PARALLEL; j += 1) {
			char d = 'q' + j;

			if (parallel)
				printf(s, "\tt.%c%d.lpush(%p(popObject2_%d_%d(t.%c%d)",
					{funcs[j], i, i, j, i, d, d});
			else
				printf(s, "\t%p(t.%c%d[0]", {funcs[j], i, d});

			for (k = 0; k < NEURON_PARALLEL; k += 1) {
				char c = 'a' + k, C = 'A' + k;
				printf(s, ",t.%c%d[0],t.%c%d[0]", {i - 1, i, c, C});
			}

			strcat(s, ",params");

			if (parallel)
				printf(s, "));\r\n", {});
			else
				printf(s, ");\r\n", {});
		}

		if (parallel) {
			printf(s, "\t]\r\n", {});
			for (k = 0; k < NEURON_PARALLEL; k += 1) {
				char c = 'a' + k, C = 'A' + k;
				printf(s, "\t%p(%p(%p(t.%c%d)), t.%c%d);\r\n",
					{pushObject, unfixObject, popObject, i - 1, i - 1, c, c});
				printf(s, "\t%p(%p(%p(t.%c%d)), t.%c%d);\r\n",
					{pushObject, unfixObject, popObject, i, i, C, C});
			}
		}
	}

	strcat(s, "\treturn res;\r\n};\r\n");


	/* compile training function */

	any func = compile(data.cdata, "netTraining", s, links);

	if (func.type == "".type) {
		char res[][];
		pushObject(func, res);
		return res[0];
	}

	netTrainingFunc arr[];
	pushObject(func, arr);
	netR.training = arr[0];


	/* create instance of netData */

	char g[];
	printf(g, "/* Source of createData, generated by %s */
%s
() -> (volatile any data) createData = {
	netData d;
	return d;
};
", {methodVersion, t});

	func = compile(data.cdata, "createData", g, {});

	if (func.type == "".type) {
		char to[][];
		pushObject(func, to);
		return to[0];
	}

	const type createDataFunc = () -> (volatile any data);
	createDataFunc to[];
	pushObject(func, to);
	any netData = to[0]();


	/* initialize float arrays */

	for (i = 0; i <= last; i += 1) {
		neuronLayer layer = net.layers[i];
		neuronLayerRuntime layerR = netR.layers[i];
		for (j = 0; j < NEURON_PARALLEL; j += 1) {
			neuronGroup group = layer.groups[j];
			neuronGroupRuntime groupR = layerR.groups[j];
			groupR.outputs = netData.l[3 * (i * NEURON_PARALLEL + j)];
			groupR.outputs.lpush(floatsFixedArray(group.nOutputs));
			groupR.derivations = netData.l[3 * (i * NEURON_PARALLEL + j) + 1];
			groupR.derivations.lpush(floatsFixedArray(group.nOutputs));
			groupR.weights = netData.l[3 * (i * NEURON_PARALLEL + j) + 2];
			groupR.weights.lpush(floatsFixedArray(group.nWeights * coeff));
			for (k = 0; k < group.nWeights; k += 1) {
				groupR.weights.l[0].f[k] = randObj.getNormal(data.rand);
			}
		}
	}
	netR.data = netData;


	return "";
};
